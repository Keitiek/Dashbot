import cv2
from ultralytics import YOLO
import numpy as np

# Load the YOLO model (YOLOv8n is used in this example)
model = YOLO('yolov8n.pt')

# Get the class names from the model
class_names = model.names  # List of class names, indexed by class ID

# Camera parameters (corrected)
FOCAL_LENGTH = 5000  # Focal length in mm (typical for See3CAM_CU20)
SENSOR_HEIGHT_MM = 30  # Correct sensor height for 1/3" sensor in mm
IMAGE_HEIGHT_PX = 480  # Image height in pixels for 640x480 resolution

# Open the camera
camera = cv2.VideoCapture(4)  # Adjust camera index if necessary
width = camera.get(cv2.CAP_PROP_FRAME_WIDTH)
height = camera.get(cv2.CAP_PROP_FRAME_HEIGHT)
print(f"Camera Resolution: {int(width)}x{int(height)}")

# Set camera resolution
camera.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

if not camera.isOpened():
    print("Error: Could not open camera.")
    exit()

while True:
    ret, frame = camera.read()
    if not ret:
        print("Error: Could not read frame.")
        break

    # Use YOLO model to detect objects
    results = model(frame)

    # Start y-coordinate for the first overlay text
    start_y = 30  # Initial vertical position for displaying the text column
    line_height = 30  # Distance between each line of text

    # Filter for specific classes (ID 0 for 'person', ID 7 for 'truck', etc.)
    detections_of_interest = []
    for result in results:
        for detection in result.boxes:
            if int(detection.cls) in range(8):  # Filter for classes of interest
                detections_of_interest.append(detection)

    # Annotate the frame with the detections
    for idx, detection in enumerate(detections_of_interest):
        # Get the bounding box coordinates
        x1, y1, x2, y2 = map(int, detection.xyxy[0])  # Bounding box coordinates

        # Get the class name dynamically based on detection.cls
        class_id = int(detection.cls)
        class_name = class_names[class_id]

        # Get the confidence score for the detection
        confidence = detection.conf.item()

        # Calculate the height of the bounding box in pixels
        object_height_in_image_px = y2 - y1

        # Safeguard: Ensure object_height_in_image_px is not zero or too small
        if object_height_in_image_px <= 0:
            print(f"Bounding box height too small: {object_height_in_image_px}px. Skipping...")
            continue

        # Estimate distance using the pinhole camera model formula
        distance = (FOCAL_LENGTH * SENSOR_HEIGHT_MM) / (object_height_in_image_px * IMAGE_HEIGHT_PX)

        # Known real-world height of a person (1.7 meters)
        KNOWN_PERSON_HEIGHT = 1.7  # Average height of a person in meters
        KNOWN_TRUCK_HEIGHT = 3.5  # Average height of a truck in meters

        # Dynamically estimate the real height of the object based on detection
        if class_name == 'person':
            real_height = KNOWN_PERSON_HEIGHT
        elif class_name == 'truck':
            real_height = KNOWN_TRUCK_HEIGHT
        else:
            real_height = (object_height_in_image_px * distance) / FOCAL_LENGTH

        # Create a label with distance and real height
        overlay_text = f'{class_name} {confidence:.2f} Distance: {distance:.2f}m Height: {real_height:.2f}m'

        # Draw the bounding box on the frame
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

        # Display the overlay text in a column, separated by 'line_height' for each object
        text_y = start_y + idx * line_height
        cv2.putText(frame, overlay_text, (10, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)

    # --- Line Detection Process ---
    # Convert the frame to HSV color space for white color segmentation
    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    
    # Define lower and upper bounds for white color in HSV space
    lower_white = np.array([0, 0, 200])  # Lower HSV values for white
    upper_white = np.array([25, 0, 255])  # Upper HSV values for white
    
    # Create a mask that isolates white regions
    white_mask = cv2.inRange(hsv_frame, lower_white, upper_white)
    
    # Apply the mask to the frame to extract only white regions
    white_frame = cv2.bitwise_and(frame, frame, mask=white_mask)
    
    # Convert the masked white frame to grayscale
    gray_white_frame = cv2.cvtColor(white_frame, cv2.COLOR_BGR2GRAY)

    # Apply Canny edge detection on the white regions
    edges = cv2.Canny(gray_white_frame, 50, 150, apertureSize=7)

    # Detect lines using the Hough Line Transform
    lines = cv2.HoughLinesP(edges, 5, np.pi / 180, threshold=100, minLineLength=40, maxLineGap=40)

    # Draw detected lines on the original frame
    if lines is not None:
        for line in lines:
            x1, y1, x2, y2 = line[0]
            cv2.line(frame, (x1, y1), (x2, y2), (0, 0, 255), 5)  # Red color for lines

    # Show the frame with the detected objects and lines
    cv2.imshow('People, Truck Detection, and White Line Detection', frame)

    # Break the loop on 'q' key press
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the camera and close the window
camera.release()
cv2.destroyAllWindows()
